{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 1 — Imports and Global Configuration (Colab Compatible)\n",
        "# ============================================================\n",
        "\n",
        "# Install OpenML (Colab does not include this library)\n",
        "!pip install --quiet openml\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Core Python\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Numerical / data handling\n",
        "# ------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# OpenML\n",
        "# ------------------------------------------------------------\n",
        "import openml\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Scikit-learn utilities\n",
        "# ------------------------------------------------------------\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Reproducibility utilities\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def set_global_seed(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Set random seeds for Python, NumPy (and any other libraries we might add).\n",
        "    Call this once at the beginning of the main execution.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Global configuration dataclass\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class AutoMLConfig:\n",
        "    \"\"\"\n",
        "    Global configuration for the AutoML pipeline.\n",
        "    Everything downstream reads values from here.\n",
        "    \"\"\"\n",
        "    # Reproducibility\n",
        "    random_seed: int = 42\n",
        "\n",
        "    # Data splitting\n",
        "    test_size: float = 0.2\n",
        "    n_splits_cv: int = 5\n",
        "\n",
        "    # Dataset-size thresholds\n",
        "    small_n_samples: int = 10_000\n",
        "    medium_n_samples: int = 100_000\n",
        "\n",
        "    # Max number of HPO trials (multi-fidelity)\n",
        "    max_trials_small: int = 100\n",
        "    max_trials_medium: int = 60\n",
        "    max_trials_large: int = 30\n",
        "\n",
        "    # HPO behavior\n",
        "    initial_random_trials: int = 20\n",
        "\n",
        "    # Fidelity levels used in BOHB-like HPO\n",
        "    fidelity_levels: List[Dict[str, Any]] = None\n",
        "\n",
        "    # Label for output CSV\n",
        "    approach_name: str = \"AutoML-BOHB\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Define fidelity levels if not set\n",
        "        if self.fidelity_levels is None:\n",
        "            self.fidelity_levels = [\n",
        "                {\"name\": \"low\",    \"row_fraction\": 0.25, \"n_splits\": 3},\n",
        "                {\"name\": \"medium\", \"row_fraction\": 0.50, \"n_splits\": 3},\n",
        "                {\"name\": \"high\",   \"row_fraction\": 1.00, \"n_splits\": self.n_splits_cv},\n",
        "            ]\n",
        "\n",
        "\n",
        "def get_default_config() -> AutoMLConfig:\n",
        "    \"\"\"\n",
        "    Return a new AutoML configuration object.\n",
        "    Used in the main execution cell.\n",
        "    \"\"\"\n",
        "    return AutoMLConfig()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Pretty printing for progress banners\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def print_banner(message: str) -> None:\n",
        "    \"\"\"\n",
        "    Print a clean, visually distinct status banner.\n",
        "    Helps Colab users track progress.\n",
        "    \"\"\"\n",
        "    bar = \"=\" * max(10, len(message) + 8)\n",
        "    print(f\"\\n{bar}\\n>>> {message}\\n{bar}\\n\")\n"
      ],
      "metadata": {
        "id": "hKlnMyLZLQwU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 2 — Dataset Retrieval and Splitting\n",
        "# ============================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetMeta:\n",
        "    \"\"\"\n",
        "    Metadata for one OpenML dataset, used throughout the pipeline.\n",
        "    \"\"\"\n",
        "    dataset_id: int\n",
        "    dataset_name: str\n",
        "    target_name: str\n",
        "\n",
        "    n_samples: int\n",
        "    n_features: int\n",
        "    n_classes: int\n",
        "\n",
        "    categorical_features: List[str]\n",
        "    numerical_features: List[str]\n",
        "\n",
        "    n_categorical: int\n",
        "    n_numerical: int\n",
        "\n",
        "    missing_value_pct: float\n",
        "    class_imbalance_ratio: float\n",
        "\n",
        "\n",
        "def load_openml_dataset(dataset_id: int) -> Tuple[pd.DataFrame, pd.Series, DatasetMeta]:\n",
        "    \"\"\"\n",
        "    Load a dataset from OpenML and compute meta-information:\n",
        "    - feature types (categorical vs numerical)\n",
        "    - basic size stats\n",
        "    - missing value percentage\n",
        "    - class imbalance ratio\n",
        "\n",
        "    Returns:\n",
        "        X: pandas DataFrame of features\n",
        "        y: pandas Series of target labels\n",
        "        meta: DatasetMeta object\n",
        "    \"\"\"\n",
        "    print(f\"Loading OpenML dataset {dataset_id} ...\")\n",
        "    dataset = openml.datasets.get_dataset(dataset_id)\n",
        "\n",
        "    # Determine target name\n",
        "    target_name = dataset.default_target_attribute\n",
        "\n",
        "    # Get data as pandas DataFrame\n",
        "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
        "        target=target_name,\n",
        "        dataset_format=\"dataframe\"\n",
        "    )\n",
        "\n",
        "    # Basic shape\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Feature type lists\n",
        "    categorical_features = [\n",
        "        name for name, is_cat in zip(attribute_names, categorical_indicator) if is_cat\n",
        "    ]\n",
        "    numerical_features = [\n",
        "        name for name, is_cat in zip(attribute_names, categorical_indicator) if not is_cat\n",
        "    ]\n",
        "    n_categorical = len(categorical_features)\n",
        "    n_numerical = len(numerical_features)\n",
        "\n",
        "    # Number of classes (assuming classification; y can be strings or numbers)\n",
        "    unique_classes = pd.unique(y)\n",
        "    n_classes = len(unique_classes)\n",
        "\n",
        "    # Missing value percentage (features only)\n",
        "    total_entries = n_samples * n_features\n",
        "    missing_count = X.isna().sum().sum()\n",
        "    missing_value_pct = (missing_count / total_entries) * 100.0 if total_entries > 0 else 0.0\n",
        "\n",
        "    # Class imbalance ratio: min_count / max_count\n",
        "    # We use value_counts to handle any label type\n",
        "    class_counts = pd.Series(y).value_counts()\n",
        "    if len(class_counts) > 0:\n",
        "        class_imbalance_ratio = float(class_counts.min() / class_counts.max())\n",
        "    else:\n",
        "        class_imbalance_ratio = 1.0  # degenerate case (shouldn't happen)\n",
        "\n",
        "    meta = DatasetMeta(\n",
        "        dataset_id=int(dataset_id),\n",
        "        dataset_name=dataset.name,\n",
        "        target_name=target_name,\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_classes=n_classes,\n",
        "        categorical_features=categorical_features,\n",
        "        numerical_features=numerical_features,\n",
        "        n_categorical=n_categorical,\n",
        "        n_numerical=n_numerical,\n",
        "        missing_value_pct=missing_value_pct,\n",
        "        class_imbalance_ratio=class_imbalance_ratio,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Loaded dataset {meta.dataset_id} ({meta.dataset_name}) \"\n",
        "        f\"with {meta.n_samples} samples, {meta.n_features} features \"\n",
        "        f\"({meta.n_numerical} numerical, {meta.n_categorical} categorical).\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Missing values: {meta.missing_value_pct:.2f}% | \"\n",
        "        f\"Class imbalance ratio: {meta.class_imbalance_ratio:.3f}\"\n",
        "    )\n",
        "\n",
        "    return X, y, meta\n",
        "\n",
        "\n",
        "def train_test_split_dataset(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    meta: DatasetMeta,\n",
        "    config: AutoMLConfig,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Split the dataset into train and test sets using a fixed random seed.\n",
        "    Uses stratification on y (classification assumption).\n",
        "\n",
        "    Args:\n",
        "        X: feature DataFrame\n",
        "        y: target Series\n",
        "        meta: DatasetMeta, used mainly for logging\n",
        "        config: AutoMLConfig with test_size and random_seed\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    print(\n",
        "        f\"Splitting dataset {meta.dataset_id} ({meta.dataset_name}) \"\n",
        "        f\"into train/test with test_size={config.test_size:.2f} ...\"\n",
        "    )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=config.test_size,\n",
        "        random_state=config.random_seed,\n",
        "        stratify=y,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Train size: {len(X_train)} samples | \"\n",
        "        f\"Test size: {len(X_test)} samples\"\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "yx-ivq4MLbZ0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 3 — Preprocessing Pipeline Construction\n",
        "# ============================================================\n",
        "\n",
        "from typing import Optional\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "\n",
        "def build_preprocessor(meta: DatasetMeta) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Build the default preprocessing pipeline based on dataset metadata.\n",
        "\n",
        "    Logic (as agreed earlier):\n",
        "      - If we know feature types:\n",
        "          * numerical: SimpleImputer(mean) + StandardScaler\n",
        "          * categorical: SimpleImputer(most_frequent) + OneHotEncoder(ignore unknown, dense)\n",
        "        -> combined via ColumnTransformer\n",
        "      - If we effectively have no type information, fall back to:\n",
        "          * SimpleImputer(mean) + StandardScaler on all columns\n",
        "    \"\"\"\n",
        "    n_num = meta.n_numerical\n",
        "    n_cat = meta.n_categorical\n",
        "\n",
        "    # If we have at least *some* feature information, use ColumnTransformer\n",
        "    if (n_num + n_cat) > 0:\n",
        "        print(\n",
        "            f\"Building preprocessing pipeline for dataset {meta.dataset_id} \"\n",
        "            f\"({meta.dataset_name}) with {n_num} numerical and {n_cat} categorical features.\"\n",
        "        )\n",
        "\n",
        "        transformers = []\n",
        "\n",
        "        if n_num > 0:\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "                (\"scaler\", StandardScaler()),\n",
        "            ])\n",
        "            transformers.append(\n",
        "                (\"num\", numeric_transformer, meta.numerical_features)\n",
        "            )\n",
        "\n",
        "        if n_cat > 0:\n",
        "            categorical_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                (\"onehot\", OneHotEncoder(\n",
        "                    handle_unknown=\"ignore\",\n",
        "                    sparse_output=False,\n",
        "                )),\n",
        "            ])\n",
        "            transformers.append(\n",
        "                (\"cat\", categorical_transformer, meta.categorical_features)\n",
        "            )\n",
        "\n",
        "        if len(transformers) == 0:\n",
        "            # Extremely unlikely: no usable features at all\n",
        "            print(\n",
        "                \"Warning: No numerical or categorical features detected; \"\n",
        "                \"falling back to simple numeric preprocessing on all columns.\"\n",
        "            )\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    (\n",
        "                        \"all\",\n",
        "                        Pipeline(steps=[\n",
        "                            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "                            (\"scaler\", StandardScaler()),\n",
        "                        ]),\n",
        "                        meta.numerical_features + meta.categorical_features,\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=transformers\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        # Fallback: no feature type information\n",
        "        print(\n",
        "            f\"Feature types unknown for dataset {meta.dataset_id} ({meta.dataset_name}); \"\n",
        "            \"using simple numeric preprocessing on all columns.\"\n",
        "        )\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\n",
        "                    \"all\",\n",
        "                    Pipeline(steps=[\n",
        "                        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "                        (\"scaler\", StandardScaler()),\n",
        "                    ]),\n",
        "                    list(range(meta.n_features)),  # assumes positional indexing\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Optional Feature Engineering Block\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def _light_feature_engineering_transform(X):\n",
        "    \"\"\"\n",
        "    A lightweight, generic feature engineering transform.\n",
        "\n",
        "    Current behavior:\n",
        "      - Takes the preprocessed feature matrix X (numpy array).\n",
        "      - Creates log1p-transformed features on non-negative values and\n",
        "        horizontally stacks them with the original features.\n",
        "\n",
        "    This roughly doubles the feature space but stays generic and safe:\n",
        "      - log1p(0) is defined,\n",
        "      - we clip X at 0 for the log transform to avoid negative issues.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    # Clip at 0 for log1p to avoid log on negative values\n",
        "    X_clipped = np.clip(X, a_min=0.0, a_max=None)\n",
        "    X_log = np.log1p(X_clipped)\n",
        "    return np.hstack([X, X_log])\n",
        "\n",
        "\n",
        "def build_feature_engineering_block(mode: str) -> FunctionTransformer:\n",
        "    \"\"\"\n",
        "    Build the feature engineering block based on the selected mode.\n",
        "\n",
        "    Modes:\n",
        "      - \"none\": identity transform (no additional features).\n",
        "      - \"light\": apply a simple log1p-based expansion on the preprocessed features.\n",
        "\n",
        "    Returns:\n",
        "        A scikit-learn-compatible transformer to be included in the pipeline.\n",
        "    \"\"\"\n",
        "    mode = (mode or \"none\").lower()\n",
        "\n",
        "    if mode == \"none\":\n",
        "        print(\"Feature engineering mode: none (identity transform).\")\n",
        "        fe_block = FunctionTransformer(\n",
        "            func=lambda X: X,\n",
        "            validate=False,\n",
        "        )\n",
        "    elif mode == \"light\":\n",
        "        print(\"Feature engineering mode: light (adding log1p-transformed features).\")\n",
        "        fe_block = FunctionTransformer(\n",
        "            func=_light_feature_engineering_transform,\n",
        "            validate=False,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown feature engineering mode: {mode!r}\")\n",
        "\n",
        "    return fe_block\n"
      ],
      "metadata": {
        "id": "MlwTPay5Lwo8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 4 — Model Search Space and Pipeline Assembly\n",
        "# ============================================================\n",
        "\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Model imports\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    HistGradientBoostingClassifier,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Model search space definition\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def get_model_search_space() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Define the joint search space for:\n",
        "      - feature_engineering_mode\n",
        "      - model_type\n",
        "      - model-specific hyperparameters\n",
        "\n",
        "    The structure is a plain Python dict, which will be interpreted by\n",
        "    the HPO routine in Cell 5.\n",
        "\n",
        "    Conventions:\n",
        "      - \"type\": one of {\"categorical\", \"int\", \"float\", \"logfloat\"}\n",
        "      - \"choices\": for categorical\n",
        "      - \"low\", \"high\": for numerical ranges\n",
        "    \"\"\"\n",
        "    search_space: Dict[str, Any] = {}\n",
        "\n",
        "    # Top-level hyperparameters\n",
        "    search_space[\"feature_engineering_mode\"] = {\n",
        "        \"type\": \"categorical\",\n",
        "        \"choices\": [\"none\", \"light\"],\n",
        "    }\n",
        "\n",
        "    search_space[\"model_type\"] = {\n",
        "        \"type\": \"categorical\",\n",
        "        \"choices\": [\n",
        "            \"logistic_regression\",\n",
        "            \"linear_svm\",\n",
        "            \"rbf_svm\",\n",
        "            \"decision_tree\",\n",
        "            \"random_forest\",\n",
        "            \"extra_trees\",\n",
        "            \"hist_gradient_boosting\",\n",
        "            \"knn\",\n",
        "            \"naive_bayes\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    # Model-specific hyperparameters\n",
        "    search_space[\"logistic_regression\"] = {\n",
        "        \"C\": {\"type\": \"logfloat\", \"low\": 1e-4, \"high\": 1e3},\n",
        "        # to keep things simple/robust we stick to l2; l1 would require solver juggling\n",
        "        \"penalty\": {\"type\": \"categorical\", \"choices\": [\"l2\"]},\n",
        "        \"class_weight\": {\"type\": \"categorical\", \"choices\": [None, \"balanced\"]},\n",
        "    }\n",
        "\n",
        "    search_space[\"linear_svm\"] = {\n",
        "        \"C\": {\"type\": \"logfloat\", \"low\": 1e-4, \"high\": 1e3},\n",
        "        \"loss\": {\"type\": \"categorical\", \"choices\": [\"hinge\", \"squared_hinge\"]},\n",
        "        \"class_weight\": {\"type\": \"categorical\", \"choices\": [None, \"balanced\"]},\n",
        "    }\n",
        "\n",
        "    search_space[\"rbf_svm\"] = {\n",
        "        \"C\": {\"type\": \"logfloat\", \"low\": 1e-3, \"high\": 1e3},\n",
        "        \"gamma\": {\"type\": \"logfloat\", \"low\": 1e-4, \"high\": 10.0},\n",
        "        \"class_weight\": {\"type\": \"categorical\", \"choices\": [None, \"balanced\"]},\n",
        "    }\n",
        "\n",
        "    search_space[\"decision_tree\"] = {\n",
        "        \"criterion\": {\"type\": \"categorical\", \"choices\": [\"gini\", \"entropy\"]},\n",
        "        \"max_depth\": {\"type\": \"int\", \"low\": 2, \"high\": 30},\n",
        "        \"min_samples_split\": {\"type\": \"int\", \"low\": 2, \"high\": 20},\n",
        "        \"min_samples_leaf\": {\"type\": \"int\", \"low\": 1, \"high\": 20},\n",
        "        \"max_features\": {\"type\": \"float\", \"low\": 0.1, \"high\": 1.0},\n",
        "    }\n",
        "\n",
        "    search_space[\"random_forest\"] = {\n",
        "        \"n_estimators\": {\"type\": \"int\", \"low\": 100, \"high\": 600},\n",
        "        \"criterion\": {\"type\": \"categorical\", \"choices\": [\"gini\", \"entropy\"]},\n",
        "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 30},\n",
        "        \"min_samples_split\": {\"type\": \"int\", \"low\": 2, \"high\": 20},\n",
        "        \"min_samples_leaf\": {\"type\": \"int\", \"low\": 1, \"high\": 20},\n",
        "        \"max_features\": {\"type\": \"float\", \"low\": 0.2, \"high\": 1.0},\n",
        "        \"bootstrap\": {\"type\": \"categorical\", \"choices\": [True, False]},\n",
        "        \"class_weight\": {\"type\": \"categorical\", \"choices\": [None, \"balanced\"]},\n",
        "    }\n",
        "\n",
        "    search_space[\"extra_trees\"] = {\n",
        "        \"n_estimators\": {\"type\": \"int\", \"low\": 100, \"high\": 600},\n",
        "        \"criterion\": {\"type\": \"categorical\", \"choices\": [\"gini\", \"entropy\"]},\n",
        "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 30},\n",
        "        \"min_samples_split\": {\"type\": \"int\", \"low\": 2, \"high\": 20},\n",
        "        \"min_samples_leaf\": {\"type\": \"int\", \"low\": 1, \"high\": 20},\n",
        "        \"max_features\": {\"type\": \"float\", \"low\": 0.2, \"high\": 1.0},\n",
        "        \"bootstrap\": {\"type\": \"categorical\", \"choices\": [True, False]},\n",
        "    }\n",
        "\n",
        "    search_space[\"hist_gradient_boosting\"] = {\n",
        "        \"learning_rate\": {\"type\": \"logfloat\", \"low\": 0.01, \"high\": 0.3},\n",
        "        \"max_depth\": {\"type\": \"int\", \"low\": 2, \"high\": 16},\n",
        "        \"max_leaf_nodes\": {\"type\": \"int\", \"low\": 16, \"high\": 256},\n",
        "        \"min_samples_leaf\": {\"type\": \"int\", \"low\": 20, \"high\": 200},\n",
        "        \"l2_regularization\": {\"type\": \"float\", \"low\": 0.0, \"high\": 1.0},\n",
        "        \"max_bins\": {\"type\": \"int\", \"low\": 64, \"high\": 255},\n",
        "    }\n",
        "\n",
        "    search_space[\"knn\"] = {\n",
        "        \"n_neighbors\": {\"type\": \"int\", \"low\": 1, \"high\": 50},\n",
        "        \"weights\": {\"type\": \"categorical\", \"choices\": [\"uniform\", \"distance\"]},\n",
        "        \"p\": {\"type\": \"categorical\", \"choices\": [1, 2]},  # Manhattan vs Euclidean\n",
        "        \"leaf_size\": {\"type\": \"int\", \"low\": 20, \"high\": 60},\n",
        "    }\n",
        "\n",
        "    search_space[\"naive_bayes\"] = {\n",
        "        # GaussianNB's var_smoothing (log-scale)\n",
        "        \"var_smoothing\": {\"type\": \"logfloat\", \"low\": 1e-12, \"high\": 1e-6},\n",
        "    }\n",
        "\n",
        "    return search_space\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Estimator factory\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def _build_estimator(model_type: str, params: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Map a (model_type, hyperparameter dict) pair to a concrete\n",
        "    scikit-learn estimator instance.\n",
        "    \"\"\"\n",
        "    if model_type == \"logistic_regression\":\n",
        "        return LogisticRegression(\n",
        "            C=params.get(\"C\", 1.0),\n",
        "            penalty=params.get(\"penalty\", \"l2\"),\n",
        "            class_weight=params.get(\"class_weight\", None),\n",
        "            solver=\"lbfgs\",\n",
        "            max_iter=1000,\n",
        "        )\n",
        "\n",
        "    if model_type == \"linear_svm\":\n",
        "        return LinearSVC(\n",
        "            C=params.get(\"C\", 1.0),\n",
        "            loss=params.get(\"loss\", \"squared_hinge\"),\n",
        "            class_weight=params.get(\"class_weight\", None),\n",
        "            max_iter=5000,\n",
        "        )\n",
        "\n",
        "    if model_type == \"rbf_svm\":\n",
        "        return SVC(\n",
        "            C=params.get(\"C\", 1.0),\n",
        "            gamma=params.get(\"gamma\", \"scale\"),\n",
        "            kernel=\"rbf\",\n",
        "            class_weight=params.get(\"class_weight\", None),\n",
        "            probability=True,  # useful for probability-based metrics later\n",
        "        )\n",
        "\n",
        "    if model_type == \"decision_tree\":\n",
        "        return DecisionTreeClassifier(\n",
        "            criterion=params.get(\"criterion\", \"gini\"),\n",
        "            max_depth=params.get(\"max_depth\", None),\n",
        "            min_samples_split=params.get(\"min_samples_split\", 2),\n",
        "            min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
        "            max_features=params.get(\"max_features\", None),\n",
        "        )\n",
        "\n",
        "    if model_type == \"random_forest\":\n",
        "        return RandomForestClassifier(\n",
        "            n_estimators=params.get(\"n_estimators\", 100),\n",
        "            criterion=params.get(\"criterion\", \"gini\"),\n",
        "            max_depth=params.get(\"max_depth\", None),\n",
        "            min_samples_split=params.get(\"min_samples_split\", 2),\n",
        "            min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
        "            max_features=params.get(\"max_features\", \"sqrt\"),\n",
        "            bootstrap=params.get(\"bootstrap\", True),\n",
        "            class_weight=params.get(\"class_weight\", None),\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "\n",
        "    if model_type == \"extra_trees\":\n",
        "        return ExtraTreesClassifier(\n",
        "            n_estimators=params.get(\"n_estimators\", 100),\n",
        "            criterion=params.get(\"criterion\", \"gini\"),\n",
        "            max_depth=params.get(\"max_depth\", None),\n",
        "            min_samples_split=params.get(\"min_samples_split\", 2),\n",
        "            min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
        "            max_features=params.get(\"max_features\", \"sqrt\"),\n",
        "            bootstrap=params.get(\"bootstrap\", False),\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "\n",
        "    if model_type == \"hist_gradient_boosting\":\n",
        "        return HistGradientBoostingClassifier(\n",
        "            learning_rate=params.get(\"learning_rate\", 0.1),\n",
        "            max_depth=params.get(\"max_depth\", None),\n",
        "            max_leaf_nodes=params.get(\"max_leaf_nodes\", 31),\n",
        "            min_samples_leaf=params.get(\"min_samples_leaf\", 20),\n",
        "            l2_regularization=params.get(\"l2_regularization\", 0.0),\n",
        "            max_bins=params.get(\"max_bins\", 255),\n",
        "        )\n",
        "\n",
        "    if model_type == \"knn\":\n",
        "        return KNeighborsClassifier(\n",
        "            n_neighbors=params.get(\"n_neighbors\", 5),\n",
        "            weights=params.get(\"weights\", \"uniform\"),\n",
        "            p=params.get(\"p\", 2),\n",
        "            leaf_size=params.get(\"leaf_size\", 30),\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "\n",
        "    if model_type == \"naive_bayes\":\n",
        "        return GaussianNB(\n",
        "            var_smoothing=params.get(\"var_smoothing\", 1e-9),\n",
        "        )\n",
        "\n",
        "    raise ValueError(f\"Unknown model_type {model_type!r}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Full pipeline builder\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_model_pipeline(\n",
        "    model_config: Dict[str, Any],\n",
        "    preprocessor,\n",
        "    fe_block,\n",
        ") -> Pipeline:\n",
        "    \"\"\"\n",
        "    Build a full scikit-learn Pipeline:\n",
        "\n",
        "        preprocessor -> feature_engineering -> model\n",
        "\n",
        "    Args:\n",
        "        model_config: dict containing at least:\n",
        "            - \"model_type\"\n",
        "            - \"feature_engineering_mode\" (the mode that's already used to build fe_block)\n",
        "            - model-specific hyperparameters\n",
        "        preprocessor: the preprocessor object (ColumnTransformer)\n",
        "        fe_block: the feature-engineering transformer\n",
        "\n",
        "    Returns:\n",
        "        pipeline: fitted-ready Pipeline object.\n",
        "    \"\"\"\n",
        "    model_type = model_config.get(\"model_type\")\n",
        "    if model_type is None:\n",
        "        raise ValueError(\"model_config must contain a 'model_type' key.\")\n",
        "\n",
        "    # Extract model-specific params (exclude non-model keys)\n",
        "    non_param_keys = {\"model_type\", \"feature_engineering_mode\"}\n",
        "    model_params = {k: v for k, v in model_config.items() if k not in non_param_keys}\n",
        "\n",
        "    estimator = _build_estimator(model_type, model_params)\n",
        "\n",
        "    pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\"feature_engineering\", fe_block),\n",
        "            (\"model\", estimator),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return pipeline\n"
      ],
      "metadata": {
        "id": "VXdfpZExMDwV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 5 — Hyperparameter Optimization (HPO)\n",
        "# ============================================================\n",
        "\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Budget regime selection\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def determine_budget_regime(meta: DatasetMeta, config: AutoMLConfig) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Decide how many HPO trials to run for this dataset based on its size.\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "            \"regime\": \"small\" | \"medium\" | \"large\",\n",
        "            \"max_trials\": int,\n",
        "            \"fidelity\": dict  # one of config.fidelity_levels\n",
        "        }\n",
        "    \"\"\"\n",
        "    n = meta.n_samples\n",
        "    if n <= config.small_n_samples:\n",
        "        regime = \"small\"\n",
        "        max_trials = config.max_trials_small\n",
        "    elif n <= config.medium_n_samples:\n",
        "        regime = \"medium\"\n",
        "        max_trials = config.max_trials_medium\n",
        "    else:\n",
        "        regime = \"large\"\n",
        "        max_trials = config.max_trials_large\n",
        "\n",
        "    # For now, we use the highest fidelity level (full data & full CV)\n",
        "    fidelity = config.fidelity_levels[-1]\n",
        "\n",
        "    print(\n",
        "        f\"Dataset {meta.dataset_id} ({meta.dataset_name}) has {n} samples → \"\n",
        "        f\"using '{regime}' budget with max_trials={max_trials} \"\n",
        "        f\"and fidelity={fidelity['name']} \"\n",
        "        f\"(row_fraction={fidelity['row_fraction']}, n_splits={fidelity['n_splits']}).\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"regime\": regime,\n",
        "        \"max_trials\": max_trials,\n",
        "        \"fidelity\": fidelity,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Random sampling from the search space\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def _sample_numeric(param_spec: Dict[str, Any]) -> Any:\n",
        "    \"\"\"Sample a numeric hyperparameter from its spec.\"\"\"\n",
        "    ptype = param_spec[\"type\"]\n",
        "    low = param_spec[\"low\"]\n",
        "    high = param_spec[\"high\"]\n",
        "\n",
        "    if ptype == \"int\":\n",
        "        return int(np.random.randint(low, high + 1))\n",
        "    elif ptype == \"float\":\n",
        "        return float(np.random.uniform(low, high))\n",
        "    elif ptype == \"logfloat\":\n",
        "        log_low = np.log(low)\n",
        "        log_high = np.log(high)\n",
        "        value = np.exp(np.random.uniform(log_low, log_high))\n",
        "        return float(value)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported numeric type: {ptype!r}\")\n",
        "\n",
        "\n",
        "def _sample_categorical(param_spec: Dict[str, Any]) -> Any:\n",
        "    \"\"\"Sample a categorical hyperparameter from its spec.\"\"\"\n",
        "    choices = param_spec[\"choices\"]\n",
        "    idx = np.random.randint(0, len(choices))\n",
        "    return choices[idx]\n",
        "\n",
        "\n",
        "def sample_random_config(search_space: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sample a full random configuration from the given search space.\n",
        "\n",
        "    The resulting config dict contains:\n",
        "      - \"feature_engineering_mode\"\n",
        "      - \"model_type\"\n",
        "      - model-type-specific hyperparameters\n",
        "    \"\"\"\n",
        "    config: Dict[str, Any] = {}\n",
        "\n",
        "    # Top-level choices\n",
        "    fe_spec = search_space[\"feature_engineering_mode\"]\n",
        "    model_type_spec = search_space[\"model_type\"]\n",
        "\n",
        "    feature_engineering_mode = _sample_categorical(fe_spec)\n",
        "    model_type = _sample_categorical(model_type_spec)\n",
        "\n",
        "    config[\"feature_engineering_mode\"] = feature_engineering_mode\n",
        "    config[\"model_type\"] = model_type\n",
        "\n",
        "    # Model-specific hyperparameters\n",
        "    model_space = search_space.get(model_type, {})\n",
        "    for param_name, param_spec in model_space.items():\n",
        "        ptype = param_spec[\"type\"]\n",
        "        if ptype in (\"int\", \"float\", \"logfloat\"):\n",
        "            value = _sample_numeric(param_spec)\n",
        "        elif ptype == \"categorical\":\n",
        "            value = _sample_categorical(param_spec)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown parameter type {ptype!r} for {param_name!r}\")\n",
        "        config[param_name] = value\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CV evaluation helper\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def _evaluate_config_cv(\n",
        "    pipeline: Pipeline,\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    n_splits: int,\n",
        "    row_fraction: float,\n",
        "    base_random_state: int,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate a pipeline via cross-validation on (a fraction of) the training data.\n",
        "\n",
        "    Args:\n",
        "        pipeline: scikit-learn Pipeline to evaluate\n",
        "        X_train, y_train: training data\n",
        "        n_splits: number of CV folds\n",
        "        row_fraction: fraction of rows to subsample (1.0 → use all)\n",
        "        base_random_state: used for subsampling and CV splitter\n",
        "\n",
        "    Returns:\n",
        "        (mean_accuracy, std_accuracy)\n",
        "    \"\"\"\n",
        "    # Subsample rows if row_fraction < 1.0\n",
        "    if row_fraction < 1.0:\n",
        "        n_total = len(X_train)\n",
        "        n_sub = max(1, int(n_total * row_fraction))\n",
        "        rng = np.random.RandomState(base_random_state)\n",
        "        idx = rng.choice(n_total, size=n_sub, replace=False)\n",
        "        X_sub = X_train.iloc[idx].reset_index(drop=True)\n",
        "        y_sub = y_train.iloc[idx].reset_index(drop=True)\n",
        "    else:\n",
        "        X_sub = X_train\n",
        "        y_sub = y_train\n",
        "\n",
        "    skf = StratifiedKFold(\n",
        "        n_splits=n_splits,\n",
        "        shuffle=True,\n",
        "        random_state=base_random_state,\n",
        "    )\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_sub, y_sub), start=1):\n",
        "        X_tr = X_sub.iloc[train_idx]\n",
        "        X_val = X_sub.iloc[val_idx]\n",
        "        y_tr = y_sub.iloc[train_idx]\n",
        "        y_val = y_sub.iloc[val_idx]\n",
        "\n",
        "        # Fit and evaluate\n",
        "        pipeline.fit(X_tr, y_tr)\n",
        "        y_pred = pipeline.predict(X_val)\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        scores.append(acc)\n",
        "\n",
        "    mean_acc = float(np.mean(scores)) if scores else 0.0\n",
        "    std_acc = float(np.std(scores)) if scores else 0.0\n",
        "    return mean_acc, std_acc\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Main HPO loop\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def run_hpo(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    meta: DatasetMeta,\n",
        "    preprocessor: ColumnTransformer,\n",
        "    search_space: Dict[str, Any],\n",
        "    budget_settings: Dict[str, Any],\n",
        "    config: AutoMLConfig,\n",
        ") -> Tuple[Dict[str, Any], float, float, int, float]:\n",
        "    \"\"\"\n",
        "    Run hyperparameter optimization for a single dataset.\n",
        "\n",
        "    Currently implemented as:\n",
        "      - Pure random search over the joint configuration space\n",
        "      - Single fidelity level (the highest one from config.fidelity_levels)\n",
        "      - Cross-validation accuracy as the objective\n",
        "\n",
        "    Args:\n",
        "        X_train, y_train: training data\n",
        "        meta: DatasetMeta\n",
        "        preprocessor: pre-built preprocessing transformer for this dataset\n",
        "        search_space: model and FE hyperparameter definitions\n",
        "        budget_settings: output of determine_budget_regime(...)\n",
        "        config: global AutoML config\n",
        "\n",
        "    Returns:\n",
        "        best_config: dict of best hyperparameters (including model_type, fe_mode)\n",
        "        best_cv_mean: float, mean CV accuracy for best_config\n",
        "        best_cv_std: float, std of CV accuracy for best_config\n",
        "        n_trials: int, number of evaluated configs\n",
        "        optimization_time: float, total time spent in HPO (seconds)\n",
        "    \"\"\"\n",
        "    max_trials = budget_settings[\"max_trials\"]\n",
        "    fidelity = budget_settings[\"fidelity\"]\n",
        "    row_fraction = fidelity[\"row_fraction\"]\n",
        "    n_splits = fidelity[\"n_splits\"]\n",
        "\n",
        "    print_banner(\n",
        "        f\"Starting HPO for dataset {meta.dataset_id} ({meta.dataset_name}) \"\n",
        "        f\"with max_trials={max_trials}\"\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    best_config: Dict[str, Any] = {}\n",
        "    best_cv_mean: float = -np.inf\n",
        "    best_cv_std: float = 0.0\n",
        "\n",
        "    # Main random search loop\n",
        "    for trial_idx in range(1, max_trials + 1):\n",
        "        # Sample a random configuration\n",
        "        config_candidate = sample_random_config(search_space)\n",
        "\n",
        "        fe_mode = config_candidate.get(\"feature_engineering_mode\", \"none\")\n",
        "        fe_block = build_feature_engineering_block(fe_mode)\n",
        "\n",
        "        # Build pipeline for this config\n",
        "        pipeline = build_model_pipeline(config_candidate, preprocessor, fe_block)\n",
        "\n",
        "        # Evaluate via CV\n",
        "        base_random_state = config.random_seed + trial_idx\n",
        "        mean_acc, std_acc = _evaluate_config_cv(\n",
        "            pipeline,\n",
        "            X_train,\n",
        "            y_train,\n",
        "            n_splits=n_splits,\n",
        "            row_fraction=row_fraction,\n",
        "            base_random_state=base_random_state,\n",
        "        )\n",
        "\n",
        "        # Update best if necessary\n",
        "        if mean_acc > best_cv_mean:\n",
        "            best_cv_mean = mean_acc\n",
        "            best_cv_std = std_acc\n",
        "            best_config = config_candidate\n",
        "\n",
        "        # Occasionally print progress\n",
        "        if (trial_idx == 1) or (trial_idx % max(1, max_trials // 5) == 0):\n",
        "            print(\n",
        "                f\"[HPO] Trial {trial_idx:3d}/{max_trials} | \"\n",
        "                f\"cv_accuracy={mean_acc:.4f} (std={std_acc:.4f}) | \"\n",
        "                f\"best={best_cv_mean:.4f} (model={best_config.get('model_type')}, \"\n",
        "                f\"fe={best_config.get('feature_engineering_mode')})\"\n",
        "            )\n",
        "\n",
        "    optimization_time = float(time.time() - start_time)\n",
        "\n",
        "    print_banner(\n",
        "        f\"HPO finished for dataset {meta.dataset_id} ({meta.dataset_name}) | \"\n",
        "        f\"best_cv_accuracy={best_cv_mean:.4f} | \"\n",
        "        f\"n_trials={max_trials} | \"\n",
        "        f\"optimization_time={optimization_time:.1f}s\"\n",
        "    )\n",
        "\n",
        "    return best_config, best_cv_mean, best_cv_std, max_trials, optimization_time\n"
      ],
      "metadata": {
        "id": "7oPVf7mnMbAs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 6 — Final Training and Test Evaluation\n",
        "# ============================================================\n",
        "\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "\n",
        "def train_final_model(\n",
        "    best_config: Dict[str, Any],\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    preprocessor: ColumnTransformer,\n",
        "    config: AutoMLConfig,\n",
        ") -> Tuple[Pipeline, float]:\n",
        "    \"\"\"\n",
        "    Train the final model on the full training data using the best configuration\n",
        "    found during HPO.\n",
        "\n",
        "    Args:\n",
        "        best_config: dict with at least:\n",
        "            - \"model_type\"\n",
        "            - \"feature_engineering_mode\"\n",
        "            - model-specific hyperparameters\n",
        "        X_train, y_train: full training data (no subsampling)\n",
        "        preprocessor: pre-built preprocessing transformer for this dataset\n",
        "        config: global configuration (used e.g. for random seed if needed)\n",
        "\n",
        "    Returns:\n",
        "        final_pipeline: fitted Pipeline (preprocessor -> FE -> model)\n",
        "        training_time: seconds spent on final training\n",
        "    \"\"\"\n",
        "    if not best_config:\n",
        "        raise ValueError(\"best_config is empty; HPO must return a non-empty configuration.\")\n",
        "\n",
        "    model_type = best_config.get(\"model_type\", \"unknown\")\n",
        "    fe_mode = best_config.get(\"feature_engineering_mode\", \"none\")\n",
        "\n",
        "    print_banner(\n",
        "        f\"Training final model for dataset with model_type={model_type}, \"\n",
        "        f\"feature_engineering_mode={fe_mode}\"\n",
        "    )\n",
        "\n",
        "    # Rebuild feature engineering block and full pipeline\n",
        "    fe_block = build_feature_engineering_block(fe_mode)\n",
        "    final_pipeline = build_model_pipeline(best_config, preprocessor, fe_block)\n",
        "\n",
        "    # Fit on full training data\n",
        "    start_time = time.time()\n",
        "    final_pipeline.fit(X_train, y_train)\n",
        "    training_time = float(time.time() - start_time)\n",
        "\n",
        "    print(\n",
        "        f\"Final model trained in {training_time:.2f} seconds \"\n",
        "        f\"(model_type={model_type}, fe_mode={fe_mode}).\"\n",
        "    )\n",
        "\n",
        "    return final_pipeline, training_time\n",
        "\n",
        "\n",
        "def evaluate_on_test(\n",
        "    final_pipeline: Pipeline,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.Series,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate the final trained pipeline on the held-out test set.\n",
        "\n",
        "    Computes:\n",
        "      - test_accuracy\n",
        "      - test_balanced_accuracy\n",
        "      - test_f1_macro\n",
        "      - test_f1_weighted\n",
        "\n",
        "    Args:\n",
        "        final_pipeline: fitted Pipeline from train_final_model\n",
        "        X_test, y_test: held-out test data\n",
        "\n",
        "    Returns:\n",
        "        dict with the four test metrics.\n",
        "    \"\"\"\n",
        "    print(\n",
        "        f\"Evaluating final model on test set with {len(X_test)} samples...\"\n",
        "    )\n",
        "\n",
        "    y_pred = final_pipeline.predict(X_test)\n",
        "\n",
        "    test_accuracy = float(accuracy_score(y_test, y_pred))\n",
        "    test_balanced_accuracy = float(balanced_accuracy_score(y_test, y_pred))\n",
        "    test_f1_macro = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "    test_f1_weighted = float(f1_score(y_test, y_pred, average=\"weighted\"))\n",
        "\n",
        "    print(\n",
        "        \"Test metrics: \"\n",
        "        f\"accuracy={test_accuracy:.4f}, \"\n",
        "        f\"balanced_accuracy={test_balanced_accuracy:.4f}, \"\n",
        "        f\"f1_macro={test_f1_macro:.4f}, \"\n",
        "        f\"f1_weighted={test_f1_weighted:.4f}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"test_accuracy\": test_accuracy,\n",
        "        \"test_balanced_accuracy\": test_balanced_accuracy,\n",
        "        \"test_f1_macro\": test_f1_macro,\n",
        "        \"test_f1_weighted\": test_f1_weighted,\n",
        "    }\n",
        "\n",
        "\n",
        "def summarize_hyperparameters(best_config: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Build:\n",
        "      - a compact, human-readable hyperparameter string\n",
        "      - a JSON-serializable dict of all hyperparameters\n",
        "\n",
        "    Excludes none of the keys from the JSON, but for the string we skip\n",
        "    overly verbose or redundant fields if needed (for now we keep all).\n",
        "\n",
        "    Args:\n",
        "        best_config: dict containing at least:\n",
        "            - \"model_type\"\n",
        "            - \"feature_engineering_mode\"\n",
        "            - model-specific hyperparameters\n",
        "\n",
        "    Returns:\n",
        "        hyperparameters_str: string, e.g. \"model=rf, fe=light, max_depth=10, n_estimators=300\"\n",
        "        hyperparameters_json: dict, a shallow copy of best_config\n",
        "    \"\"\"\n",
        "    if not best_config:\n",
        "        return \"\", {}\n",
        "\n",
        "    model_type = best_config.get(\"model_type\", \"unknown\")\n",
        "    fe_mode = best_config.get(\"feature_engineering_mode\", \"none\")\n",
        "\n",
        "    # Human-readable string\n",
        "    # Show model_type + fe_mode first, then sorted hyperparameters\n",
        "    non_param_keys = {\"model_type\", \"feature_engineering_mode\"}\n",
        "    param_items = sorted(\n",
        "        [(k, v) for k, v in best_config.items() if k not in non_param_keys],\n",
        "        key=lambda kv: kv[0],\n",
        "    )\n",
        "\n",
        "    param_str_parts = [f\"{k}={v}\" for k, v in param_items]\n",
        "    params_str = \", \".join(param_str_parts)\n",
        "\n",
        "    hyperparameters_str = (\n",
        "        f\"model_type={model_type}, feature_engineering_mode={fe_mode}\"\n",
        "        + (\", \" + params_str if params_str else \"\")\n",
        "    )\n",
        "\n",
        "    # JSON/dict version: just copy best_config\n",
        "    hyperparameters_json = dict(best_config)\n",
        "\n",
        "    return hyperparameters_str, hyperparameters_json\n"
      ],
      "metadata": {
        "id": "w4bS-suCNJ8V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 7 — Result Row Construction and Saving\n",
        "# ============================================================\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "\n",
        "\n",
        "def build_result_row(\n",
        "    meta: DatasetMeta,\n",
        "    best_config: Dict[str, Any],\n",
        "    cv_mean: float,\n",
        "    cv_std: float,\n",
        "    test_metrics: Dict[str, float],\n",
        "    optimization_time: float,\n",
        "    training_time: float,\n",
        "    n_trials: int,\n",
        "    config: AutoMLConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Build a single result-row dict matching the benchmark_results.csv format:\n",
        "\n",
        "        dataset_id\n",
        "        dataset_name\n",
        "        approach\n",
        "        model_type\n",
        "        hyperparameters\n",
        "        cv_accuracy_mean\n",
        "        cv_accuracy_std\n",
        "        test_accuracy\n",
        "        test_balanced_accuracy\n",
        "        test_f1_macro\n",
        "        test_f1_weighted\n",
        "        optimization_time\n",
        "        training_time\n",
        "        n_trials\n",
        "        feature_engineering\n",
        "        rationale\n",
        "        hyperparameters_json\n",
        "\n",
        "    Args:\n",
        "        meta: DatasetMeta for this dataset\n",
        "        best_config: dict of best hyperparameters (includes model_type, fe_mode)\n",
        "        cv_mean, cv_std: cross-validation accuracy statistics for best_config\n",
        "        test_metrics: dict with test_* metrics\n",
        "        optimization_time: total HPO time in seconds\n",
        "        training_time: final model training time in seconds\n",
        "        n_trials: number of evaluated configurations\n",
        "        config: global AutoMLConfig (provides approach_name, etc.)\n",
        "\n",
        "    Returns:\n",
        "        A dict with keys exactly matching the desired CSV columns.\n",
        "    \"\"\"\n",
        "    # Derive hyperparameters string + JSON\n",
        "    hyperparameters_str, hyperparameters_json = summarize_hyperparameters(best_config)\n",
        "\n",
        "    model_type = best_config.get(\"model_type\", \"unknown\")\n",
        "    fe_mode = best_config.get(\"feature_engineering_mode\", \"none\")\n",
        "\n",
        "    # We don't use LLM rationales here → leave empty string\n",
        "    rationale_str = \"\"\n",
        "\n",
        "    # Serialize JSON dict to a string for CSV storage\n",
        "    hyperparameters_json_str = json.dumps(hyperparameters_json)\n",
        "\n",
        "    row: Dict[str, Any] = {\n",
        "        \"dataset_id\": meta.dataset_id,\n",
        "        \"dataset_name\": meta.dataset_name,\n",
        "        \"approach\": config.approach_name,\n",
        "        \"model_type\": model_type,\n",
        "        \"hyperparameters\": hyperparameters_str,\n",
        "        \"cv_accuracy_mean\": float(cv_mean),\n",
        "        \"cv_accuracy_std\": float(cv_std),\n",
        "        \"test_accuracy\": float(test_metrics[\"test_accuracy\"]),\n",
        "        \"test_balanced_accuracy\": float(test_metrics[\"test_balanced_accuracy\"]),\n",
        "        \"test_f1_macro\": float(test_metrics[\"test_f1_macro\"]),\n",
        "        \"test_f1_weighted\": float(test_metrics[\"test_f1_weighted\"]),\n",
        "        \"optimization_time\": float(optimization_time),\n",
        "        \"training_time\": float(training_time),\n",
        "        \"n_trials\": int(n_trials),\n",
        "        \"feature_engineering\": fe_mode,\n",
        "        \"rationale\": rationale_str,\n",
        "        \"hyperparameters_json\": hyperparameters_json_str,\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        f\"Built result row for dataset {meta.dataset_id} ({meta.dataset_name}): \"\n",
        "        f\"test_accuracy={row['test_accuracy']:.4f}, model_type={model_type}, fe={fe_mode}\"\n",
        "    )\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "def save_results(results_rows: List[Dict[str, Any]], output_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a list of result-row dicts into a DataFrame and save to CSV.\n",
        "\n",
        "    Ensures the column order matches the benchmark_results.csv file:\n",
        "        dataset_id\n",
        "        dataset_name\n",
        "        approach\n",
        "        model_type\n",
        "        hyperparameters\n",
        "        cv_accuracy_mean\n",
        "        cv_accuracy_std\n",
        "        test_accuracy\n",
        "        test_balanced_accuracy\n",
        "        test_f1_macro\n",
        "        test_f1_weighted\n",
        "        optimization_time\n",
        "        training_time\n",
        "        n_trials\n",
        "        feature_engineering\n",
        "        rationale\n",
        "        hyperparameters_json\n",
        "\n",
        "    Args:\n",
        "        results_rows: list of result-row dicts from build_result_row\n",
        "        output_path: path to the CSV file to write\n",
        "\n",
        "    Returns:\n",
        "        The pandas DataFrame that was saved.\n",
        "    \"\"\"\n",
        "    if not results_rows:\n",
        "        print(\"No results to save — results_rows is empty.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    columns = [\n",
        "        \"dataset_id\",\n",
        "        \"dataset_name\",\n",
        "        \"approach\",\n",
        "        \"model_type\",\n",
        "        \"hyperparameters\",\n",
        "        \"cv_accuracy_mean\",\n",
        "        \"cv_accuracy_std\",\n",
        "        \"test_accuracy\",\n",
        "        \"test_balanced_accuracy\",\n",
        "        \"test_f1_macro\",\n",
        "        \"test_f1_weighted\",\n",
        "        \"optimization_time\",\n",
        "        \"training_time\",\n",
        "        \"n_trials\",\n",
        "        \"feature_engineering\",\n",
        "        \"rationale\",\n",
        "        \"hyperparameters_json\",\n",
        "    ]\n",
        "\n",
        "    df = pd.DataFrame(results_rows)\n",
        "\n",
        "    # Reorder columns to match the expected output format\n",
        "    df = df[columns]\n",
        "\n",
        "    print_banner(f\"Saving results to {output_path}\")\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Saved {len(df)} rows to {output_path}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "zgfi9qUnNUlM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 8 — Main Pipeline Execution\n",
        "# ============================================================\n",
        "\n",
        "# 1. User specifies dataset IDs here\n",
        "dataset_ids = [ 1459\n",
        "]\n",
        "# 2. Load global config\n",
        "config = get_default_config()\n",
        "set_global_seed(config.random_seed)\n",
        "\n",
        "print_banner(\"Starting Full AutoML Pipeline\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# 3. Loop over datasets\n",
        "for dataset_id in dataset_ids:\n",
        "    print_banner(f\"Processing dataset {dataset_id}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Load dataset\n",
        "    # -------------------------\n",
        "    X, y, meta = load_openml_dataset(dataset_id)\n",
        "\n",
        "    # -------------------------\n",
        "    # Train/test split\n",
        "    # -------------------------\n",
        "    X_train, X_test, y_train, y_test = train_test_split_dataset(\n",
        "        X, y, meta, config\n",
        "    )\n",
        "\n",
        "    # -------------------------\n",
        "    # Build preprocessing pipeline\n",
        "    # -------------------------\n",
        "    preprocessor = build_preprocessor(meta)\n",
        "\n",
        "    # -------------------------\n",
        "    # Model search space\n",
        "    # -------------------------\n",
        "    search_space = get_model_search_space()\n",
        "\n",
        "    # -------------------------\n",
        "    # Determine budget for this dataset\n",
        "    # -------------------------\n",
        "    budget_settings = determine_budget_regime(meta, config)\n",
        "\n",
        "    # -------------------------\n",
        "    # Run HPO (random-search version)\n",
        "    # -------------------------\n",
        "    (\n",
        "        best_config,\n",
        "        cv_mean,\n",
        "        cv_std,\n",
        "        n_trials,\n",
        "        optimization_time,\n",
        "    ) = run_hpo(\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        meta=meta,\n",
        "        preprocessor=preprocessor,\n",
        "        search_space=search_space,\n",
        "        budget_settings=budget_settings,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # -------------------------\n",
        "    # Train final model\n",
        "    # -------------------------\n",
        "    final_pipeline, training_time = train_final_model(\n",
        "        best_config, X_train, y_train, preprocessor, config\n",
        "    )\n",
        "\n",
        "    # -------------------------\n",
        "    # Test evaluation\n",
        "    # -------------------------\n",
        "    test_metrics = evaluate_on_test(final_pipeline, X_test, y_test)\n",
        "\n",
        "    # -------------------------\n",
        "    # Assemble one-row result\n",
        "    # -------------------------\n",
        "    result_row = build_result_row(\n",
        "        meta=meta,\n",
        "        best_config=best_config,\n",
        "        cv_mean=cv_mean,\n",
        "        cv_std=cv_std,\n",
        "        test_metrics=test_metrics,\n",
        "        optimization_time=optimization_time,\n",
        "        training_time=training_time,\n",
        "        n_trials=n_trials,\n",
        "        config=config,\n",
        "    )\n",
        "    all_results.append(result_row)\n",
        "\n",
        "    print_banner(\n",
        "        f\"Finished dataset {dataset_id} — \"\n",
        "        f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\"\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Save all results to CSV\n",
        "# -----------------------------\n",
        "output_path = \"automl_pipeline_results.csv\"\n",
        "df_results = save_results(all_results, output_path)\n",
        "\n",
        "print_banner(\"AutoML Pipeline Completed\")\n",
        "df_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2CHKkhPANjyE",
        "outputId": "7237ce19-4bbe-4623-817d-e529feffd99f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====================================\n",
            ">>> Starting Full AutoML Pipeline\n",
            "=====================================\n",
            "\n",
            "\n",
            "===============================\n",
            ">>> Processing dataset 1459\n",
            "===============================\n",
            "\n",
            "Loading OpenML dataset 1459 ...\n",
            "Loaded dataset 1459 (artificial-characters) with 10218 samples, 7 features (7 numerical, 0 categorical).\n",
            "Missing values: 0.00% | Class imbalance ratio: 0.424\n",
            "Splitting dataset 1459 (artificial-characters) into train/test with test_size=0.20 ...\n",
            "Train size: 8174 samples | Test size: 2044 samples\n",
            "Building preprocessing pipeline for dataset 1459 (artificial-characters) with 7 numerical and 0 categorical features.\n",
            "Dataset 1459 (artificial-characters) has 10218 samples → using 'medium' budget with max_trials=60 and fidelity=high (row_fraction=1.0, n_splits=5).\n",
            "\n",
            "================================================================================\n",
            ">>> Starting HPO for dataset 1459 (artificial-characters) with max_trials=60\n",
            "================================================================================\n",
            "\n",
            "Feature engineering mode: none (identity transform).\n",
            "[HPO] Trial   1/60 | cv_accuracy=0.6502 (std=0.0087) | best=0.6502 (model=decision_tree, fe=none)\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "[HPO] Trial  12/60 | cv_accuracy=0.2645 (std=0.0107) | best=0.8340 (model=knn, fe=none)\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "[HPO] Trial  24/60 | cv_accuracy=0.3916 (std=0.0111) | best=0.8340 (model=knn, fe=none)\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "[HPO] Trial  36/60 | cv_accuracy=0.7671 (std=0.0113) | best=0.8722 (model=hist_gradient_boosting, fe=light)\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "[HPO] Trial  48/60 | cv_accuracy=0.3444 (std=0.0080) | best=0.8722 (model=hist_gradient_boosting, fe=light)\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Feature engineering mode: none (identity transform).\n",
            "Feature engineering mode: none (identity transform).\n",
            "[HPO] Trial  60/60 | cv_accuracy=0.2561 (std=0.0057) | best=0.8722 (model=hist_gradient_boosting, fe=light)\n",
            "\n",
            "================================================================================================================================\n",
            ">>> HPO finished for dataset 1459 (artificial-characters) | best_cv_accuracy=0.8722 | n_trials=60 | optimization_time=680.8s\n",
            "================================================================================================================================\n",
            "\n",
            "\n",
            "===============================================================================================================\n",
            ">>> Training final model for dataset with model_type=hist_gradient_boosting, feature_engineering_mode=light\n",
            "===============================================================================================================\n",
            "\n",
            "Feature engineering mode: light (adding log1p-transformed features).\n",
            "Final model trained in 3.70 seconds (model_type=hist_gradient_boosting, fe_mode=light).\n",
            "Evaluating final model on test set with 2044 samples...\n",
            "Test metrics: accuracy=0.9095, balanced_accuracy=0.9036, f1_macro=0.9031, f1_weighted=0.9095\n",
            "Built result row for dataset 1459 (artificial-characters): test_accuracy=0.9095, model_type=hist_gradient_boosting, fe=light\n",
            "\n",
            "=====================================================\n",
            ">>> Finished dataset 1459 — Test Accuracy: 0.9095\n",
            "=====================================================\n",
            "\n",
            "\n",
            "=====================================================\n",
            ">>> Saving results to automl_pipeline_results.csv\n",
            "=====================================================\n",
            "\n",
            "Saved 1 rows to automl_pipeline_results.csv\n",
            "\n",
            "=================================\n",
            ">>> AutoML Pipeline Completed\n",
            "=================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   dataset_id           dataset_name     approach              model_type  \\\n",
              "0        1459  artificial-characters  AutoML-BOHB  hist_gradient_boosting   \n",
              "\n",
              "                                     hyperparameters  cv_accuracy_mean  \\\n",
              "0  model_type=hist_gradient_boosting, feature_eng...          0.872155   \n",
              "\n",
              "   cv_accuracy_std  test_accuracy  test_balanced_accuracy  test_f1_macro  \\\n",
              "0         0.009642       0.909491                0.903619       0.903054   \n",
              "\n",
              "   test_f1_weighted  optimization_time  training_time  n_trials  \\\n",
              "0          0.909516         680.779557       3.702307        60   \n",
              "\n",
              "  feature_engineering rationale  \\\n",
              "0               light             \n",
              "\n",
              "                                hyperparameters_json  \n",
              "0  {\"feature_engineering_mode\": \"light\", \"model_t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-255b0741-1e51-45f1-a39f-220ad2ddd3c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset_id</th>\n",
              "      <th>dataset_name</th>\n",
              "      <th>approach</th>\n",
              "      <th>model_type</th>\n",
              "      <th>hyperparameters</th>\n",
              "      <th>cv_accuracy_mean</th>\n",
              "      <th>cv_accuracy_std</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>test_balanced_accuracy</th>\n",
              "      <th>test_f1_macro</th>\n",
              "      <th>test_f1_weighted</th>\n",
              "      <th>optimization_time</th>\n",
              "      <th>training_time</th>\n",
              "      <th>n_trials</th>\n",
              "      <th>feature_engineering</th>\n",
              "      <th>rationale</th>\n",
              "      <th>hyperparameters_json</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1459</td>\n",
              "      <td>artificial-characters</td>\n",
              "      <td>AutoML-BOHB</td>\n",
              "      <td>hist_gradient_boosting</td>\n",
              "      <td>model_type=hist_gradient_boosting, feature_eng...</td>\n",
              "      <td>0.872155</td>\n",
              "      <td>0.009642</td>\n",
              "      <td>0.909491</td>\n",
              "      <td>0.903619</td>\n",
              "      <td>0.903054</td>\n",
              "      <td>0.909516</td>\n",
              "      <td>680.779557</td>\n",
              "      <td>3.702307</td>\n",
              "      <td>60</td>\n",
              "      <td>light</td>\n",
              "      <td></td>\n",
              "      <td>{\"feature_engineering_mode\": \"light\", \"model_t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-255b0741-1e51-45f1-a39f-220ad2ddd3c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-255b0741-1e51-45f1-a39f-220ad2ddd3c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-255b0741-1e51-45f1-a39f-220ad2ddd3c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_c4f925fa-b03d-4ef9-adcd-34df6344d6cb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c4f925fa-b03d-4ef9-adcd-34df6344d6cb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"dataset_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1459,\n        \"max\": 1459,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1459\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"artificial-characters\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approach\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AutoML-BOHB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hist_gradient_boosting\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hyperparameters\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"model_type=hist_gradient_boosting, feature_engineering_mode=light, l2_regularization=0.28328440349257733, learning_rate=0.11696730464663162, max_bins=188, max_depth=11, max_leaf_nodes=45, min_samples_leaf=37\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cv_accuracy_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8721554579856938,\n        \"max\": 0.8721554579856938,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8721554579856938\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cv_accuracy_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.009641634381155041,\n        \"max\": 0.009641634381155041,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.009641634381155041\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9094911937377691,\n        \"max\": 0.9094911937377691,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9094911937377691\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_balanced_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9036186493348983,\n        \"max\": 0.9036186493348983,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9036186493348983\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9030536626887825,\n        \"max\": 0.9030536626887825,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9030536626887825\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_f1_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9095164743675638,\n        \"max\": 0.9095164743675638,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9095164743675638\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"optimization_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 680.7795565128326,\n        \"max\": 680.7795565128326,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          680.7795565128326\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3.7023074626922607,\n        \"max\": 3.7023074626922607,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3.7023074626922607\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_trials\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 60,\n        \"max\": 60,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          60\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_engineering\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"light\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rationale\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hyperparameters_json\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"{\\\"feature_engineering_mode\\\": \\\"light\\\", \\\"model_type\\\": \\\"hist_gradient_boosting\\\", \\\"learning_rate\\\": 0.11696730464663162, \\\"max_depth\\\": 11, \\\"max_leaf_nodes\\\": 45, \\\"min_samples_leaf\\\": 37, \\\"l2_regularization\\\": 0.28328440349257733, \\\"max_bins\\\": 188}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}