dataset_id,dataset_name,approach,model_type,hyperparameters,cv_accuracy_mean,cv_accuracy_std,test_accuracy,test_balanced_accuracy,test_f1_macro,test_f1_weighted,optimization_time,training_time,feature_engineering,rationale,pipeline_str,hyperparameters_json,feature_engineering_json
37,diabetes,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.7245182724252492,0.036313196622103854,0.7597402597402597,0.7212962962962963,0.7274597024919883,0.7554767899150163,0.0,2.3322389125823975,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
37,diabetes,tpot,LogisticRegression,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.7414548726467332,0.02359389005786257,0.7532467532467533,0.7461111111111112,0.7372485632183908,0.7566147932527243,304.91031312942505,0.002045154571533203,,TPOT selected LogisticRegression after 304.9s evolutionary search,"Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),
                ('passthrough', Passthrough()),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('featureunion',
                                                 FeatureUnion(transformer_list=[('binarizer',
                                                                                 Binarizer(threshold=0.8852524022289)),
                                                                                ('columnordinalencoder',
                                                                                 ColumnOrdinalEncoder())])),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('logisticregression',
                 LogisticRegression(C=27949.744736288492,
                                    class_weight='balanced', max_iter=1000,
                                    n_jobs=1, penalty=np.str_('l1'),
                                    random_state=42, solver='saga'))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
37,diabetes,llm,XGBClassifier,"{'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 0.8}",0.7024307862679955,0.029521981118702315,0.7207792207792207,0.6912962962962963,0.6921002464313943,0.72016902983565,9.694545030593872,0.453540563583374,"[{'name': 'PolynomialFeatures', 'operation': 'polynomial', 'parameters': {'degree': 2}}, {'name': 'SelectKBest', 'operation': 'feature_selection', 'parameters': {'method': 'model_based', 'k_features': 0.9}}]","The diabetes dataset has 768 samples with 8 numerical features and moderate class imbalance (0.54 balance ratio). Given the small feature set and potential non-linear relationships in medical data, polynomial features (degree 2) can capture interactions between clinical measurements. Feature selection (keeping 90% of features) prevents overfitting from the expanded feature space. XGBoost was chosen because it handles class imbalance well, provides feature importance insights, and typically outperforms other algorithms on tabular medical datasets of this size. The hyperparameters balance complexity (200 estimators, depth 5) with regularization (subsample 0.8) to prevent overfitting on the limited dataset.",,"{""n_estimators"": 200, ""learning_rate"": 0.1, ""max_depth"": 5, ""subsample"": 0.8}","[{""name"": ""PolynomialFeatures"", ""operation"": ""polynomial"", ""parameters"": {""degree"": 2}}, {""name"": ""SelectKBest"", ""operation"": ""feature_selection"", ""parameters"": {""method"": ""model_based"", ""k_features"": 0.9}}]"
61,iris,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.95,0.016666666666666698,0.9,0.9,0.899749373433584,0.8997493734335839,0.0,0.3627619743347168,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
61,iris,tpot,KNeighborsClassifier,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.975,0.03333333333333334,0.9666666666666667,0.9666666666666667,0.9665831244778613,0.9665831244778613,304.44255685806274,0.0032634735107421875,,TPOT selected KNeighborsClassifier after 304.4s evolutionary search,"Pipeline(steps=[('normalizer', Normalizer(norm=np.str_('l2'))),
                ('variancethreshold',
                 VarianceThreshold(threshold=0.0008825132477)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('featureunion',
                                                 FeatureUnion(transformer_list=[('quantiletransformer',
                                                                                 QuantileTransformer(n_quantiles=77,
                                                                                                     output_distribution=np.str_('uniform'),
                                                                                                     random_state=42)),
                                                                                ('zerocount',
                                                                                 ZeroCount())])),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('kneighborsclassifier',
                 KNeighborsClassifier(n_jobs=1, n_neighbors=3,
                                      weights=np.str_('uniform')))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
61,iris,llm,RandomForestClassifier,"{'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'max_features': 'sqrt'}",0.95,0.016666666666666698,0.9,0.9,0.899749373433584,0.8997493734335839,6.42813515663147,0.0970768928527832,[],"The Iris dataset is small (150 samples, 4 features) with well-separated classes and perfect balance. Given the dataset's simplicity and Fisher's original linear separation success, complex feature engineering is unnecessary and could lead to overfitting. Random Forest was chosen for its robustness, interpretability, and ability to capture the non-linear relationships between the two non-linearly separable classes. The hyperparameters are conservative to prevent overfitting on this small dataset while maintaining good performance. The default preprocessing (scaling and encoding) is sufficient for this clean, numerical dataset.",,"{""n_estimators"": 100, ""max_depth"": 5, ""min_samples_split"": 2, ""max_features"": ""sqrt""}",
44,spambase,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.949925777021803,0.015515501661473013,0.9446254071661238,0.9384164222873901,0.9415886019989032,0.9444084925113223,0.0,1.8172149658203125,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
44,spambase,tpot,BernoulliNB,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.8959192825112108,0.011891407468707108,0.9001085776330076,0.8944577742231701,0.8952166600712307,0.9000102476317658,352.61526823043823,0.018796682357788086,,TPOT selected BernoulliNB after 352.6s evolutionary search,"Pipeline(steps=[('minmaxscaler', MinMaxScaler()),
                ('selectfwe', SelectFwe(alpha=0.0006973008821)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('featureunion',
                                                 FeatureUnion(transformer_list=[('kbinsdiscretizer',
                                                                                 KBinsDiscretizer(encode='onehot-dense',
                                                                                                  n_bins=24,
                                                                                                  random_state=42,
                                                                                                  strategy=np.str_('uniform')))])),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('bernoullinb',
                 BernoulliNB(alpha=0.5316530115272, fit_prior=np.False_))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
44,spambase,llm,XGBClassifier,"{'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 8, 'subsample': 0.8}",0.9455790938611411,0.01876837632616451,0.9402823018458197,0.9367576053793063,0.937388524875871,0.9402384567402146,8.549561023712158,0.3013575077056885,"[{'name': 'SelectKBest', 'operation': 'feature_selection', 'parameters': {'method': 'univariate', 'k_features': 30}}]","The Spambase dataset has 57 numerical features with no missing values and good class balance. Given the moderate feature count (57) and sample size (4,601), feature selection is beneficial to reduce noise and computational complexity while maintaining performance. XGBoost was chosen because it typically excels on tabular data with numerical features, handles non-linear relationships well, and provides excellent performance on binary classification tasks like spam detection. The hyperparameters balance model complexity (max_depth=8) with regularization (subsample=0.8) to prevent overfitting while capturing complex patterns in the email feature space.",,"{""n_estimators"": 200, ""learning_rate"": 0.1, ""max_depth"": 8, ""subsample"": 0.8}","[{""name"": ""SelectKBest"", ""operation"": ""feature_selection"", ""parameters"": {""method"": ""univariate"", ""k_features"": 30}}]"
1510,wdbc,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.9594943240454075,0.03503114178309276,0.9736842105263158,0.9642857142857143,0.9712773998488284,0.9734654095556351,0.0,2.3925387859344482,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
1510,wdbc,tpot,KNeighborsClassifier,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.9594427244582043,0.02269584248682512,0.956140350877193,0.9454365079365079,0.9521289997480473,0.9557756825927252,305.1762716770172,0.04942440986633301,,TPOT selected KNeighborsClassifier after 305.2s evolutionary search,"Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),
                ('passthrough', Passthrough()),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('kneighborsclassifier',
                 KNeighborsClassifier(n_jobs=1, n_neighbors=6, p=1,
                                      weights=np.str_('distance')))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
1510,wdbc,llm,XGBClassifier,"{'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 0.8}",0.9600619195046439,0.032605631761037854,0.9736842105263158,0.9692460317460317,0.9715828832571666,0.9736164257756981,8.509938955307007,0.2519979476928711,"[{'name': 'PCA', 'operation': 'pca', 'parameters': {'n_components': 0.95}}]","The WDBC dataset has 30 highly correlated numerical features from cell nucleus measurements. PCA is beneficial to reduce multicollinearity and capture the most informative components while maintaining 95% variance. XGBoost was selected because: (1) It excels with tabular medical data and can capture complex non-linear relationships in cell morphology features, (2) The dataset size (569 samples) is sufficient for gradient boosting without overfitting, (3) It provides feature importance for medical interpretability, (4) It handles class imbalance well (0.59 balance ratio). The hyperparameters balance performance and generalization with moderate depth and learning rate.",,"{""n_estimators"": 100, ""learning_rate"": 0.1, ""max_depth"": 5, ""subsample"": 0.8}","[{""name"": ""PCA"", ""operation"": ""pca"", ""parameters"": {""n_components"": 0.95}}]"
23,cmc,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.5134418163194854,0.034572629614742105,0.5050847457627119,0.47189588936735555,0.47247818243869144,0.5003706155192459,0.0,2.436537027359009,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
23,cmc,tpot,DecisionTreeClassifier,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.5512541056120236,0.051828906495820164,0.5796610169491525,0.5732936624069196,0.5710556572306406,0.5823050598517424,307.2747805118561,0.003081798553466797,,TPOT selected DecisionTreeClassifier after 307.3s evolutionary search,"Pipeline(steps=[('normalizer', Normalizer(norm=np.str_('max'))),
                ('selectpercentile',
                 SelectPercentile(percentile=42.0854501357106)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('featureunion',
                                                 FeatureUnion(transformer_list=[('columnordinalencoder',
                                                                                 ColumnOrdinalEncoder())])),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('decisiontreeclassifier',
                 DecisionTreeClassifier(class_weight='balanced',
                                        criterion=np.str_('entropy'),
                                        max_depth=5, min_samples_leaf=14,
                                        min_samples_split=8,
                                        random_state=42))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
23,cmc,llm,RandomForestClassifier,"{'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'max_features': 'sqrt'}",0.5231163407728341,0.017551427447101545,0.5694915254237288,0.5372414305689108,0.5377842364213926,0.5647429947414725,9.241948366165161,0.2717595100402832,"[{'name': 'SelectKBest', 'operation': 'feature_selection', 'parameters': {'method': 'univariate', 'k_features': 8}}]","Given the dataset characteristics - 1,473 samples with 9 features (2 numerical, 7 categorical) and 3 classes with moderate imbalance - I selected a conservative feature engineering approach. Feature selection helps identify the most relevant features from the mixed feature types while avoiding overfitting. Random Forest is ideal for this scenario as it handles mixed data types well, is robust to feature scaling, and performs reliably on medium-sized datasets with categorical features. The hyperparameters balance model complexity (200 estimators, depth 10) with regularization (min_samples_split=5) to prevent overfitting while capturing meaningful patterns in the demographic data.",,"{""n_estimators"": 200, ""max_depth"": 10, ""min_samples_split"": 5, ""max_features"": ""sqrt""}","[{""name"": ""SelectKBest"", ""operation"": ""feature_selection"", ""parameters"": {""method"": ""univariate"", ""k_features"": 8}}]"
53,heart-statlog,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.8055263157894735,0.04590775038150746,0.8148148148148148,0.8166666666666667,0.8137931034482759,0.8153256704980844,0.0,0.39527201652526855,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
53,heart-statlog,tpot,GaussianNB,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.8507456140350877,0.0778862163429077,0.8333333333333334,0.8375,0.8328173374613003,0.8338493292053664,304.3150315284729,0.0021677017211914062,,TPOT selected GaussianNB after 304.3s evolutionary search,"Pipeline(steps=[('robustscaler',
                 RobustScaler(quantile_range=(0.1455707623918,
                                              0.9080343739241))),
                ('selectfwe', SelectFwe(alpha=0.0092050412207)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('featureunion',
                                                 FeatureUnion(transformer_list=[('binarizer',
                                                                                 Binarizer(threshold=0.8895424555543))])),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('gaussiannb', GaussianNB())])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
53,heart-statlog,llm,XGBClassifier,"{'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 0.8}",0.8149122807017545,0.06122860428058406,0.7962962962962963,0.8,0.7956656346749226,0.7969269579176699,8.85273003578186,0.2893950939178467,"[{'name': 'PolynomialFeatures', 'operation': 'polynomial', 'parameters': {'degree': 2}}, {'name': 'SelectKBest', 'operation': 'feature_selection', 'parameters': {'method': 'model_based', 'k_features': 0.9}}]","With 270 samples and 13 numerical features, this is a small-to-medium dataset where feature interactions could be important for heart disease prediction. Polynomial features (degree 2) capture potential non-linear relationships between clinical measurements. Feature selection (keeping 90% of features) prevents overfitting from the expanded feature space. XGBoost is ideal for this medical classification task as it handles mixed feature types well, provides feature importance insights, and typically outperforms other algorithms on tabular data with meaningful interactions between clinical variables.",,"{""n_estimators"": 100, ""learning_rate"": 0.1, ""max_depth"": 5, ""subsample"": 0.8}","[{""name"": ""PolynomialFeatures"", ""operation"": ""polynomial"", ""parameters"": {""degree"": 2}}, {""name"": ""SelectKBest"", ""operation"": ""feature_selection"", ""parameters"": {""method"": ""model_based"", ""k_features"": 0.9}}]"
41,glass,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.6788344988344988,0.12655494080909158,0.813953488372093,0.7055555555555556,0.7130494505494506,0.8066317403526707,0.0,2.3500523567199707,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
41,glass,tpot,KNeighborsClassifier,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.7317521367521367,0.043858893747814036,0.7441860465116279,0.6976190476190475,0.6688290204419237,0.7388522954914554,304.7073163986206,0.002791881561279297,,TPOT selected KNeighborsClassifier after 304.7s evolutionary search,"Pipeline(steps=[('standardscaler', StandardScaler()),
                ('selectpercentile',
                 SelectPercentile(percentile=72.2416055198836)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('kneighborsclassifier',
                 KNeighborsClassifier(n_jobs=1, n_neighbors=1, p=1,
                                      weights=np.str_('uniform')))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
41,glass,llm,RandomForestClassifier,"{'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'max_features': 'sqrt'}",0.6308935508935509,0.06448973468265087,0.7209302325581395,0.4984126984126984,0.4790322580645161,0.6776444111027756,8.588207006454468,0.22474956512451172,"[{'name': 'PCA', 'operation': 'pca', 'parameters': {'n_components': 0.95}}]","The glass dataset has 214 samples with 9 numerical features and 6 imbalanced classes. Given the small sample size and multi-class classification, I chose PCA to reduce potential multicollinearity among the chemical composition features while preserving 95% of variance. Random Forest is ideal because it handles multi-class problems well, is robust to feature scaling, and performs reliably on small-to-medium datasets. The hyperparameters prioritize model capacity (200 estimators, depth 10) while using feature bagging (sqrt) to reduce overfitting risk. The class imbalance (0.12) suggests ensemble methods like Random Forest will be more stable than linear models.",,"{""n_estimators"": 200, ""max_depth"": 10, ""min_samples_split"": 2, ""max_features"": ""sqrt""}","[{""name"": ""PCA"", ""operation"": ""pca"", ""parameters"": {""n_components"": 0.95}}]"
39,ecoli,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.7023044186087664,0.12368138481892578,0.8676470588235294,0.883387445887446,0.8731643924626381,0.8706799678935901,0.0,0.40612292289733887,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
39,ecoli,tpot,GaussianNB,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.5767099768186724,0.09337450133854369,0.8676470588235294,0.8928571428571429,0.8663271880265636,0.8629342236448548,304.3969609737396,0.0022177696228027344,,TPOT selected GaussianNB after 304.4s evolutionary search,"Pipeline(steps=[('robustscaler',
                 RobustScaler(quantile_range=(0.1615293644697,
                                              0.9192461615448))),
                ('selectfwe', SelectFwe(alpha=0.0329462895387)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('gaussiannb', GaussianNB())])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
39,ecoli,llm,RandomForestClassifier,"{'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'max_features': 'sqrt'}",0.7127515439471961,0.09810366423735777,0.9117647058823529,0.9241071428571429,0.9124412476855149,0.9120747002689411,9.671626567840576,0.24547648429870605,"[{'name': 'PCA', 'operation': 'pca', 'parameters': {'n_components': 0.95}}]","The ecoli dataset has 336 samples with 7 numerical features and 8 classes, making it a small dataset with high class imbalance (0.01 balance score). Given the small feature space (only 7 features), extensive feature engineering like polynomial features would risk overfitting. PCA with 95% variance retention helps reduce any potential multicollinearity while preserving most information. Random Forest is ideal because: (1) it handles multi-class classification well, (2) is robust to class imbalance, (3) works effectively with small datasets, and (4) captures complex relationships without requiring extensive feature engineering. The hyperparameters prioritize model capacity (200 estimators, depth 10) while maintaining generalization through feature subsetting (sqrt).",,"{""n_estimators"": 200, ""max_depth"": 10, ""min_samples_split"": 2, ""max_features"": ""sqrt""}","[{""name"": ""PCA"", ""operation"": ""pca"", ""parameters"": {""n_components"": 0.95}}]"
40,sonar,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.8236111111111111,0.05988672434760829,0.8809523809523809,0.875,0.8775510204081632,0.8785228377065111,0.0,0.46391892433166504,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
40,sonar,tpot,KNeighborsClassifier,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.8632843137254902,0.041907778288929845,0.8333333333333334,0.8272727272727273,0.8285714285714285,0.8299319727891158,307.9592866897583,0.06378459930419922,,TPOT selected KNeighborsClassifier after 308.0s evolutionary search,"Pipeline(steps=[('minmaxscaler', MinMaxScaler()),
                ('variancethreshold',
                 VarianceThreshold(threshold=0.0055465599844)),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('featureunion',
                                                 FeatureUnion(transformer_list=[('quantiletransformer',
                                                                                 QuantileTransformer(n_quantiles=148,
                                                                                                     output_distribution=np.str_('normal'),
                                                                                                     random_state=42)),
                                                                                ('pca',
                                                                                 PCA(n_components=0.8014254760484))])),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('kneighborsclassifier',
                 KNeighborsClassifier(n_jobs=1, n_neighbors=1, p=1,
                                      weights=np.str_('uniform')))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
40,sonar,llm,SVC,"{'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}",0.8823611111111109,0.03942977035342024,0.8571428571428571,0.8545454545454545,0.8558352402745995,0.8564890487087284,8.644334077835083,0.007451534271240234,"[{'name': 'PCA', 'operation': 'pca', 'parameters': {'n_components': 0.95}}]","The Sonar dataset has 60 numerical features with only 208 samples, creating a high-dimensional problem with risk of overfitting. PCA is essential to reduce dimensionality while preserving 95% of variance. SVC with RBF kernel is ideal because: (1) it handles non-linear relationships well (sonar signals likely have complex patterns), (2) works effectively with reduced feature sets after PCA, (3) performs well on small-to-medium sized datasets, and (4) the class balance (0.87) is sufficient for SVM without needing special balancing techniques. The high feature-to-sample ratio makes dimensionality reduction critical before applying any model.",,"{""C"": 10, ""kernel"": ""rbf"", ""gamma"": ""scale""}","[{""name"": ""PCA"", ""operation"": ""pca"", ""parameters"": {""n_components"": 0.95}}]"
1459,artificial-characters,baseline,RandomForestClassifier,"{'n_estimators': 100, 'random_state': 42}",0.8507912377869872,0.008980395569891896,0.9178082191780822,0.9129195242074924,0.9133979037671823,0.9175788676251124,0.0,2.7775533199310303,,"Default RandomForest with 100 trees, no hyperparameter tuning",,"{""n_estimators"": 100, ""random_state"": 42}",
1459,artificial-characters,tpot,KNeighborsClassifier,"{'search_space': 'linear-light', 'max_time_mins': 5}",0.6272104770679807,0.002400300765222072,0.6443248532289628,0.6426600942476688,0.6386971950406108,0.6433240389919035,350.9843978881836,0.0204010009765625,,TPOT selected KNeighborsClassifier after 351.0s evolutionary search,"Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),
                ('passthrough', Passthrough()),
                ('featureunion-1',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('featureunion-2',
                 FeatureUnion(transformer_list=[('skiptransformer',
                                                 SkipTransformer()),
                                                ('passthrough',
                                                 Passthrough())])),
                ('kneighborsclassifier',
                 KNeighborsClassifier(n_jobs=1, n_neighbors=7,
                                      weights=np.str_('uniform')))])","{""search_space"": ""linear-light"", ""max_time_mins"": 5}",
1459,artificial-characters,llm,RandomForestClassifier,"{'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2, 'max_features': 'sqrt'}",0.8526972702276975,0.010124575965792127,0.9134050880626223,0.9091589108332636,0.9095335792848488,0.913277802210443,8.409533739089966,1.6985764503479004,[],"The dataset has 10,218 samples with only 7 numerical features and 10 balanced classes. Given the small number of features and the character recognition domain, feature engineering is unnecessary as the original features likely capture the structural information well. Random Forest is ideal for this multi-class classification problem as it handles complex decision boundaries well without overfitting on small feature sets. The chosen hyperparameters provide sufficient model complexity (200 trees, depth 20) to capture the intricate patterns in character shapes while maintaining generalization through feature randomization (sqrt). The dataset size supports these parameters without risk of overfitting.",,"{""n_estimators"": 200, ""max_depth"": 20, ""min_samples_split"": 2, ""max_features"": ""sqrt""}",
